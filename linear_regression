import os
import pandas
import numpy as np
import matplotlib.pylab as plt
from sklearn import linear_model

"""
这个程序是用来模拟梯度下降法。
1 在windows下安装scipy
百度经验里面有详细说明：http://jingyan.baidu.com/article/ca41422f27c56a1eae99ed39.html

"""


# 计算误差函数
def calc_err_val(X, y, theta):
    """这个用来计算误差值
    :param
    X: is the value of real input
    y: is the value of real result
    theta: is the argument, and is what we want to calculate

    It use the Error Function = sum((X * theta.T - y)^2)/(2*m)
    """

    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))


def graidentDescent(X, y, theta, alpha, iters):
    """
    This function implement the gradient descent algorithm

    :param X: is the real input
    :param y: is the real output
    :param theta: is the argument which we want to calculate
    :param alpha: is the step we use in iterations
    :param iters: is the number of iterations
    :return: the cost
    """

    temp = np.matrix(np.zeros(theta.shape))
    # ravel 将矩阵中所有元素编程一个1*n的矩阵，这样就把矩阵横过来
    # shape[1] 取出所有元素个数
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)


    # 计算iters轮数
    for i in range(iters):
        # 计算简单误差，（非平方）
        error = (X * theta.T) - y

        # print(X)
        # print(X[:, 1])
        # 对于每个parameter计算，即theta个数
        for j in range(parameters):
            term = np.multiply(error, X[:, j])
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp
        cost[i] = calc_err_val(X, y, theta)

    return theta, cost


if __name__ == '__main__':
    # 设置路径，为./data/ex1data1.txt。之所有这么用是为了跨平台
    path = os.path.join('data', 'ex1data1.txt')

    # 从path读读取数据，有两列，名字为Population和Profit。这里names长度与数据读取长度一致
    # header是指首行是原来第几行
    data = pandas.read_csv(path, header=None, names=['Population', 'Profit'])

    # 在第一列插入，名字是'Ones', 值是1
    # 这个1是那个常量，对应x永远为1
    data.insert(0, 'Ones', 1)
    # print(data)

    # get the number of columns
    cols = data.shape[1]

    # get first col and second col
    # the first parameter refer to the first dimension
    # the second parameter refer to the second dimension
    X = data.iloc[:, 0: cols-1]
    # get last col
    y = data.iloc[:, cols-1: cols]

    # X.values is 2D array which stores the value of X
    # the type of X.values is nd array
    # after statement, X is the matrix
    X = np.matrix(X.values)
    y = np.matrix(y.values)

    # theta is a 1 * (cols-1) matrix which all values are 0
    theta = np.matrix(np.zeros((1, cols-1)))

    cost = calc_err_val(X, y, theta)

    alpha = 0.01
    iters = 1000

    g, cost = graidentDescent(X, y, theta, alpha, iters)

    x = np.linspace(data.Population.min(), data.Population.max(), 100)
    f = g[0, 0] + (g[0, 1] * x)

    fig, ax = plt.subplots(figsize=(12, 8))
    ax.plot(x, f, 'r', label='Prediction')
    ax.scatter(data.Population, data.Profit, label='Training Data')
    ax.legend(loc=2)
    ax.set_xlabel('Population')
    ax.set_ylabel('Profit')
    ax.set_title('Predicted Profit vs. Population Size')

    fig, ax = plt.subplots(figsize=(12, 8))
    ax.plot(np.arange(iters), cost, 'r')
    ax.set_xlabel('Iteration')
    ax.set_ylabel('Cost')
    ax.set_title('Error vs. Training Epoch')

    # 使用sklearn的算法计算
    model = linear_model.LinearRegression()
    model.fit(X, y)

    x = np.array(X[:, 1].A1)

    # 预测结果
    f = model.predict(X).flatten()

    # 可视化
    fig, ax = plt.subplots(figsize=(12, 8))
    ax.plot(x, f, 'r', label='Predict')
    ax.scatter(data.Population, data.Profit, label='Training Data')
    ax.legend(loc=2)
    ax.set_xlabel('Population')
    ax.set_ylabel('Profit')
    ax.set_title('Predicted Profit vs. Population Size(using sklearn)')
    plt.show()










